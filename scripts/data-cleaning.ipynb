{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c7a80a",
   "metadata": {},
   "source": [
    "# Andmetega tutvumine ja andmestiku puhastamine\n",
    "Andmed on kogutud bakalaureusetaseme ning integreeritud õppe tudengite kohta Tartu Ülikooli õppeinfosüsteemi poolt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64418a",
   "metadata": {},
   "source": [
    "### 1. Ettevalmistused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3603d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sqlite3 import Timestamp\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b950f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with timestamp None\n"
     ]
    }
   ],
   "source": [
    "# Esialgse andmestiku sisselugemine\n",
    "with open (\"../data/df_all_from_db.csv\", mode='r', encoding='utf-8') as f:\n",
    "    timestamp_line = f.readline().strip()\n",
    "    timestamp = timestamp_line.removeprefix(\"#\").lstrip() if timestamp_line.startswith(\"#\") else None\n",
    "df = pd.read_csv(\"../data/df_all_from_db.csv\", sep=',', skiprows=1 if timestamp else None)\n",
    "\n",
    "print(f\"Loaded data with timestamp {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fd52d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48524, 46)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridade ja veergude arv\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d821244",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 46)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656369bb",
   "metadata": {},
   "source": [
    "### 2. Abimeetodite defineerimine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eaba5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna(df_col, new_values):\n",
    "    na_count_before = df_col.isna().values.sum()\n",
    "    df_col.fillna(new_values, inplace=True)\n",
    "    na_count_after = df_col.isna().values.sum()\n",
    "    return na_count_before - na_count_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc97539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_timestamp(filename, timestamp):\n",
    "    print(f\"Output timestamp to {filename}\")\n",
    "    with open(filename, mode='w') as f:\n",
    "        f.write(timestamp or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278c97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(dataframe):\n",
    "    duplicates = [(k, v) for k, v in Counter(dataframe['study.place.UUID']).most_common() if v > 1]\n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"Warning: {len(duplicates)} study place UUID-s appear more than once in the dataset\")\n",
    "    \n",
    "    missing_values = df.isna().values.sum()\n",
    "    if missing_values > 0:\n",
    "        raise AssertionError(f\"{missing_values} missing values in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce05c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(filename, description, dataframe, *, index=False, rowcount=True):\n",
    "    print(f\"Output {description} to {filename}\" + (f\" ({len(dataframe)} rows)\" if rowcount else \"\"))\n",
    "    dataframe.to_csv(filename, index=index)\n",
    "    return len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f90cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting training and prediction data for each study level into separate files\n",
    "def output_data(df, study_level, timestamp):\n",
    "    # Create table describing data (cumulative metrics, missing values, etc)\n",
    "    desc = df.describe(include='all').transpose()\n",
    "    desc.insert(1, 'missing_count', len(df) - desc['count'])\n",
    "\n",
    "    Path(f\"./{study_level}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    write_data(f\"./{study_level}/df_describe.csv\", \"descriptions\", desc, index=True, rowcount=False)\n",
    "    write_timestamp(f\"./{study_level}/df_timestamp.txt\", timestamp)\n",
    "\n",
    "    total = 0\n",
    "    total += write_data(f\"./{study_level}/df_training.csv\", \"training data\", df[df[\"active\"] != 1])\n",
    "    total += write_data(f\"./{study_level}/df_prediction.csv\", \"prediction data\", df[df[\"active\"] == 1])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed41055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_data_2(df, timestamp):\n",
    "    # Create table describing data (cumulative metrics, missing values, etc)\n",
    "    desc = df.describe(include='all').transpose()\n",
    "    desc.insert(1, 'missing_count', len(df) - desc['count'])\n",
    "\n",
    "\n",
    "    write_data(\"df_describe.csv\", \"descriptions\", desc, index=True, rowcount=False)\n",
    "    write_timestamp(\"df_timestamp.txt\", timestamp)\n",
    "\n",
    "    total = 0\n",
    "    \n",
    "    total += write_data(\"df_cleaned.csv\", \"cleaned data\", df)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1d2bd",
   "metadata": {},
   "source": [
    "### 3. Andmestiku puhastamine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e2249",
   "metadata": {},
   "source": [
    "#### 3.1. Teadaolevalt puuduvate (NaN) väärtuste täitmine\n",
    "*Filling known missing values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1732b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 828 missing values in admission.special.conditions with 0.\n"
     ]
    }
   ],
   "source": [
    "filled_special_conditions = fillna(df['admission.special.conditions'], 0)\n",
    "print(f\"Filled {filled_special_conditions} missing values in admission.special.conditions with 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f37ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 862 missing values in normalized_score with curriculum average.\n",
      "Filled 284 missing values in normalized_score with faculty average.\n"
     ]
    }
   ],
   "source": [
    "norm_score_mean_per_curriculum = df.groupby('code.of.curriculum')['normalized_score'].transform('mean')\n",
    "norm_score_mean_per_faculty = df.groupby('faculty.code')['normalized_score'].transform('mean')\n",
    "filled_curriculum = fillna(df['normalized_score'], norm_score_mean_per_curriculum)\n",
    "filled_faculty = fillna(df['normalized_score'], norm_score_mean_per_faculty)\n",
    "print(f\"Filled {filled_curriculum} missing values in normalized_score with curriculum average.\")\n",
    "print(f\"Filled {filled_faculty} missing values in normalized_score with faculty average.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce603d",
   "metadata": {},
   "source": [
    "#### 3.2. Uue veeru lisamine varasemate lõpetamata uuringute arvu jaoks\n",
    "*Adding a new column that contains information about the number of previous unfinished studies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5065345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nr.of.previous.unfinished'] = df['nr.of.previous.studies.in.UT'] - df['nr.of.previous.finished']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2333da9",
   "metadata": {},
   "source": [
    "#### 3.3. Enne esimese semestri algust eksmatrikuleeritud tudengitele vastavate ridade eemaldamine\n",
    "*Some missing values appear for people who were exmatriculated before the first semester began. Just dropping them for now.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be224b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2 rows which contained missing values in semester_current column. Final length 48522 rows.\n"
     ]
    }
   ],
   "source": [
    "full_len = len(df)\n",
    "df.dropna(subset=['semester_current'], inplace=True)\n",
    "final_len = len(df)\n",
    "print(f\"Removed {full_len - final_len} rows which contained missing values in semester_current column. Final length {final_len} rows.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c7e56",
   "metadata": {},
   "source": [
    "### 4. Identifikaatori lisamine\n",
    "*Adding a new identifier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc5f0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().rename(columns={'index':'id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b971a2",
   "metadata": {},
   "source": [
    "### 5. Andmete valideerimine\n",
    "*Data validation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6caed191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 7 study place UUID-s appear more than once in the dataset\n"
     ]
    }
   ],
   "source": [
    "validate_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d65db",
   "metadata": {},
   "source": [
    "### 6. Klasside tasakaalustamatuse kontrollimine\n",
    "*Checking whether there is a class imbalance.*\n",
    "\n",
    "For imbalanced datasets, maximizing F-measure is better than maximizing accuracy. For example, majority\n",
    "class classifier has F-measure = 0.\n",
    "How to learn on imbalanced data?\n",
    "1. Undersampling – get rid of as many instances of dominant class as needed to roughly balance the\n",
    "dataset. By doing this, we lose a lot of data.\n",
    "2. Oversampling – make multiple copies of instances of the smaller class so that it would roughly balance\n",
    "the dataset.\n",
    "3. Augmentation – make slightly modified copies of instances of the smaller class so that it would roughly\n",
    "balance the dataset.\n",
    "4. Choosing an adequate performance estimate (for example F-measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e334263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dropout\n",
       "0    40389\n",
       "1     8133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['dropout']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4f8c4",
   "metadata": {},
   "source": [
    "### 7. Andmete eksportimine\n",
    "*Output data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb622ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output descriptions to ./../data/bachelor/df_describe.csv\n",
      "Output timestamp to ./../data/bachelor/df_timestamp.txt\n",
      "Output training data to ./../data/bachelor/df_training.csv (21692 rows)\n",
      "Output prediction data to ./../data/bachelor/df_prediction.csv (8712 rows)\n",
      "Output descriptions to ./../data/master/df_describe.csv\n",
      "Output timestamp to ./../data/master/df_timestamp.txt\n",
      "Output training data to ./../data/master/df_training.csv (13777 rows)\n",
      "Output prediction data to ./../data/master/df_prediction.csv (4341 rows)\n",
      "Output 48522 / 48522 rows in separate directories based on study level\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total += output_data(df[df['study.level'].isin([\"511 Bachelor's programmes\", \"503 Integrated Bachelor's and Master's programmes\", \"514 Professional higher education programmes\"])], \"../data/bachelor\", timestamp)\n",
    "total += output_data(df[df['study.level'] == \"614 Master's programmes\"], \"../data/master\", timestamp)\n",
    "#total += output_data_2(df, timestamp)\n",
    "print(f\"Output {total} / {final_len} rows in separate directories based on study level\")\n",
    "#print(f\"Output {total} / {final_len} rows\")\n",
    "assert(total == final_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
